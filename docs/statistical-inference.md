# Statistical inference
Most experiments face some fundamental complications:
- The measurements include some form of random variation or noise.
- Only a small sample is measured, from which you want to draw conclusions at large.

These complications make it difficult to conclude anything with absolute certainty. Thus, **being able to quantify how certain you are about your inferred conclusions is a crucial skill.** A huge amount of *non-reproducability in the scientific literature likely stems from a general lack of understanding of how to deal with uncertainty.* Familiarizing yourself with the topics below will provide you a solid foundation for dealing with uncertainty when analyzing data.

1. Probability distributions
2. Probability distributions for distinct types of random behavior (e.g., normal, exponential, poisson, binomial)
3. Joint and conditional probability
4. Maximum likelihood
5. Bayes Theorem
6. Sampling distribution
7. Central limit theorem
8. Confidence interval
9. Resampling (e.g., bootstrap, permutation)
10. Hypothesis testing
    - Logical implications of assuming a null hypothesis to be true
    - p-value (what it is and what it isn't) and the fact that rare events can occur (yes, even to you)

Other resources of interest:

- :bangbang: [Misuse of statistics](https://en.wikipedia.org/wiki/Misuse_of_statistics)
